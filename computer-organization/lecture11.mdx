---
title: 第11讲：并行计算
description: 并行计算架构、多核处理器、线程级并行和向量化
---

# 第11讲：并行计算

## 并行计算概述

并行计算是一种计算模式，通过同时执行多个计算任务来提高计算性能和效率。随着摩尔定律的放缓，并行计算成为提升计算性能的主要途径。

### 并行计算的基本概念

```c
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <stdint.h>
#include <stdbool.h>
#include <time.h>
#include <pthread.h>
#include <math.h>

// 并行计算类型
typedef enum {
    PARALLEL_BIT_LEVEL,      // 位级并行
    PARALLEL_INSTRUCTION_LEVEL, // 指令级并行
    PARALLEL_DATA_LEVEL,     // 数据级并行
    PARALLEL_TASK_LEVEL,     // 任务级并行
    PARALLEL_THREAD_LEVEL,   // 线程级并行
    PARALLEL_PROCESS_LEVEL   // 进程级并行
} ParallelType;

// 并行架构类型
typedef enum {
    ARCH_SISD,               // 单指令单数据
    ARCH_SIMD,               // 单指令多数据
    ARCH_MISD,               // 多指令单数据
    ARCH_MIMD,               // 多指令多数据
    ARCH_VECTOR              // 向量处理器
} ParallelArchitecture;

// 并行粒度
typedef enum {
    GRANULARITY_FINE,        // 细粒度并行
    GRANULARITY_MEDIUM,      // 中粒度并行
    GRANULARITY_COARSE       // 粗粒度并行
} ParallelGranularity;

// 并行性能指标
typedef struct {
    double speedup;           // 加速比
    double efficiency;        // 效率
    double scalability;       // 可扩展性
    double overhead;          // 开销
    uint32_t processors;      // 处理器数量
    uint32_t tasks;           // 任务数量
    double serial_time;       // 串行执行时间
    double parallel_time;     // 并行执行时间
} ParallelMetrics;

// 并行工作负载
typedef struct {
    uint32_t total_work;      // 总工作量
    uint32_t parallel_work;   // 可并行工作量
    uint32_t serial_work;     // 串行工作量
    double parallel_fraction; // 并行比例
    uint32_t task_size;       // 任务大小
    uint32_t dependencies;    // 依赖关系数
} ParallelWorkload;

// 并行计算配置
typedef struct {
    ParallelType type;        // 并行类型
    ParallelArchitecture arch; // 架构类型
    ParallelGranularity granularity; // 粒度
    uint32_t num_processors;  // 处理器数量
    uint32_t memory_size;     // 内存大小
    uint32_t cache_size;      // 缓存大小
    bool shared_memory;       // 共享内存
    bool distributed_memory; // 分布式内存
} ParallelConfig;

// 并行计算分析器
typedef struct {
    ParallelConfig config;
    ParallelMetrics metrics;
    ParallelWorkload workload;
    double overhead_factor;   // 开销因子
    double communication_factor; // 通信因子
    double synchronization_factor; // 同步因子
} ParallelAnalyzer;

// 初始化并行计算配置
ParallelConfig* init_parallel_config(ParallelType type, uint32_t num_processors) {
    ParallelConfig* config = malloc(sizeof(ParallelConfig));
    config->type = type;
    config->arch = ARCH_MIMD;
    config->granularity = GRANULARITY_MEDIUM;
    config->num_processors = num_processors;
    config->memory_size = 1024 * 1024 * 1024;  // 1GB
    config->cache_size = 32 * 1024;          // 32KB
    config->shared_memory = true;
    config->distributed_memory = false;
    return config;
}

// Amdahl定律计算
double calculate_amdahl_speedup(double parallel_fraction, uint32_t num_processors) {
    return 1.0 / ((1.0 - parallel_fraction) + parallel_fraction / num_processors);
}

// Gustafson定律计算
double calculate_gustafson_speedup(double parallel_fraction, uint32_t num_processors) {
    return num_processors - (1.0 - parallel_fraction) * (num_processors - 1);
}

// 计算并行效率
double calculate_efficiency(double speedup, uint32_t num_processors) {
    return speedup / num_processors;
}

// 计算可扩展性
double calculate_scalability(uint32_t base_processors, uint32_t target_processors,
                            double base_speedup, double target_speedup) {
    return (target_speedup / target_processors) / (base_speedup / base_processors);
}

// 分析并行架构
void analyze_parallel_architectures() {
    printf("并行架构分析\n");
    printf("============\n");

    struct {
        ParallelArchitecture arch;
        const char* name;
        const char* description;
        const char* examples;
        double efficiency_factor;
    } architectures[] = {
        {ARCH_SISD, "SISD", "单指令单数据", "早期计算机", 1.0},
        {ARCH_SIMD, "SIMD", "单指令多数据", "向量处理器、GPU", 0.9},
        {ARCH_MISD, "MISD", "多指令单数据", "容错系统", 0.7},
        {ARCH_MIMD, "MIMD", "多指令多数据", "多核处理器", 0.8},
        {ARCH_VECTOR, "向量", "向量处理", "超级计算机", 0.95}
    };

    printf("架构\t名称\t描述\t\t\t示例\t\t效率因子\n");
    printf("----\t----\t----\t\t\t----\t\t----\n");

    for (int i = 0; i < 5; i++) {
        printf("%s\t%s\t%s\t%s\t%.2f\n",
               architectures[i].name, architectures[i].name,
               architectures[i].description, architectures[i].examples,
               architectures[i].efficiency_factor);
    }
}

// 分析并行类型
void analyze_parallel_types() {
    printf("\n并行类型分析\n");
    printf("============\n");

    struct {
        ParallelType type;
        const char* name;
        const char* description;
        const char* applications;
        double overhead;
    } types[] = {
        {PARALLEL_BIT_LEVEL, "位级并行", "同时处理多位数据", "ALU设计", 0.1},
        {PARALLEL_INSTRUCTION_LEVEL, "指令级并行", "同时执行多条指令", "超标量处理器", 0.3},
        {PARALLEL_DATA_LEVEL, "数据级并行", "对不同数据执行相同操作", "GPU、SIMD", 0.2},
        {PARALLEL_TASK_LEVEL, "任务级并行", "同时执行不同任务", "多任务系统", 0.4},
        {PARALLEL_THREAD_LEVEL, "线程级并行", "多线程执行", "多核处理器", 0.5},
        {PARALLEL_PROCESS_LEVEL, "进程级并行", "多进程执行", "分布式系统", 0.6}
    };

    printf("类型\t名称\t描述\t\t\t应用\t\t开销\n");
    printf("----\t----\t----\t\t\t----\t\t----\n");

    for (int i = 0; i < 6; i++) {
        printf("%s\t%s\t%s\t%s\t%.2f\n",
               types[i].name, types[i].name,
               types[i].description, types[i].applications,
               types[i].overhead);
    }
}

// 并行性能定律分析
void analyze_parallel_laws() {
    printf("\n并行性能定律分析\n");
    printf("=================\n");

    // Amdahl定律分析
    printf("Amdahl定律：\n");
    printf("公式：S(n) = 1 / ((1 - f) + f/n)\n");
    printf("其中：f为并行比例，n为处理器数量\n\n");

    double parallel_fractions[] = {0.5, 0.7, 0.8, 0.9, 0.95, 0.99};
    uint32_t processor_counts[] = {2, 4, 8, 16, 32, 64};

    printf("并行比例\\处理器数");
    for (int i = 0; i < 6; i++) {
        printf("\t%u", processor_counts[i]);
    }
    printf("\n");

    for (int i = 0; i < 6; i++) {
        printf("%.2f\t\t", parallel_fractions[i]);
        for (int j = 0; j < 6; j++) {
            double speedup = calculate_amdahl_speedup(parallel_fractions[i], processor_counts[j]);
            printf("%.2f\t", speedup);
        }
        printf("\n");
    }

    // Gustafson定律分析
    printf("\nGustafson定律：\n");
    printf("公式：S(n) = n - (1 - f)(n - 1)\n");
    printf("强调：问题规模随处理器数量增加而增加\n\n");

    printf("并行比例\\处理器数");
    for (int i = 0; i < 6; i++) {
        printf("\t%u", processor_counts[i]);
    }
    printf("\n");

    for (int i = 0; i < 6; i++) {
        printf("%.2f\t\t", parallel_fractions[i]);
        for (int j = 0; j < 6; j++) {
            double speedup = calculate_gustafson_speedup(parallel_fractions[i], processor_counts[j]);
            printf("%.2f\t", speedup);
        }
        printf("\n");
    }
}

// 并行粒度分析
void analyze_parallel_granularity() {
    printf("\n并行粒度分析\n");
    printf("============\n");

    struct {
        ParallelGranularity granularity;
        const char* name;
        const char* description;
        const char* task_size;
        const char* overhead_level;
        double communication_ratio;
    } granularities[] = {
        {GRANULARITY_FINE, "细粒度", "指令级并行", "指令", "低", 0.1},
        {GRANULARITY_MEDIUM, "中粒度", "循环级并行", "循环", "中", 0.3},
        {GRANULARITY_COARSE, "粗粒度", "函数级并行", "函数", "高", 0.5}
    };

    printf("粒度\t名称\t描述\t\t任务大小\t开销\t通信比例\n");
    printf("----\t----\t----\t\t----\t----\t----\n");

    for (int i = 0; i < 3; i++) {
        printf("%s\t%s\t%s\t%s\t%s\t%.2f\n",
               granularities[i].name, granularities[i].name,
               granularities[i].description, granularities[i].task_size,
               granularities[i].overhead_level, granularities[i].communication_ratio);
    }

    printf("\n粒度选择原则：\n");
    printf("1. 细粒度：适合数据并行，通信开销小\n");
    printf("2. 中粒度：平衡通信和计算，通用性强\n");
    printf("3. 粗粒度：适合任务并行，减少同步开销\n");
}

// 并行编程模型
void analyze_parallel_programming_models() {
    printf("\n并行编程模型\n");
    printf("==============\n");

    struct {
        const char* model;
        const char* description;
        const char* characteristics;
        const char* applications;
        double learning_curve;
    } models[] = {
        {"OpenMP", "共享内存并行编程", "编译器指令，易用", "多核科学计算", 0.3},
        {"MPI", "消息传递接口", "进程间通信，可扩展", "分布式系统", 0.7},
        {"CUDA", "GPU并行编程", "SIMT架构，高性能", "图形、科学计算", 0.8},
        {"Pthreads", "POSIX线程", "底层线程控制", "系统编程", 0.6},
        {"OpenCL", "开放计算语言", "跨平台异构计算", "各种加速器", 0.9}
    };

    printf("模型\t描述\t\t特性\t\t应用\t\t学习曲线\n");
    printf("----\t----\t\t----\t\t----\t\t----\n");

    for (int i = 0; i < 5; i++) {
        printf("%s\t%s\t%s\t%s\t%.1f\n",
               models[i].model, models[i].description,
               models[i].characteristics, models[i].applications,
               models[i].learning_curve);
    }
}

// 主函数
int main() {
    printf("并行计算分析器\n");
    printf("=============\n\n");

    // 初始化并行计算分析器
    ParallelAnalyzer analyzer;
    analyzer.config = *init_parallel_config(PARALLEL_THREAD_LEVEL, 8);
    analyzer.workload.total_work = 1000;
    analyzer.workload.parallel_work = 800;
    analyzer.workload.serial_work = 200;
    analyzer.workload.parallel_fraction = 0.8;
    analyzer.overhead_factor = 0.1;
    analyzer.communication_factor = 0.05;
    analyzer.synchronization_factor = 0.03;

    printf("并行计算配置：\n");
    printf("并行类型: 线程级并行\n");
    printf("处理器数量: %u\n", analyzer.config.num_processors);
    printf("并行比例: %.2f\n", analyzer.workload.parallel_fraction);
    printf("总工作量: %u\n", analyzer.workload.total_work);

    // 分析并行架构
    analyze_parallel_architectures();

    // 分析并行类型
    analyze_parallel_types();

    // 分析并行性能定律
    analyze_parallel_laws();

    // 分析并行粒度
    analyze_parallel_granularity();

    // 分析并行编程模型
    analyze_parallel_programming_models();

    // 计算性能指标
    printf("\n性能计算示例：\n");
    printf("===============\n");

    uint32_t processors[] = {1, 2, 4, 8, 16, 32};
    printf("处理器数\tAmdahl加速比\tGustafson加速比\t效率\n");
    printf("--------\t--------\t--------\t----\n");

    for (int i = 0; i < 6; i++) {
        double amdahl_speedup = calculate_amdahl_speedup(
            analyzer.workload.parallel_fraction, processors[i]);
        double gustafson_speedup = calculate_gustafson_speedup(
            analyzer.workload.parallel_fraction, processors[i]);
        double efficiency = calculate_efficiency(amdahl_speedup, processors[i]);

        printf("%u\t\t%.2f\t\t%.2f\t\t%.2f\n",
               processors[i], amdahl_speedup, gustafson_speedup, efficiency);
    }

    free(&analyzer.config);

    return 0;
}
```

### 并行计算的关键挑战

1. **负载均衡**：如何平衡各个处理单元的工作负载
2. **通信开销**：处理单元间的数据交换成本
3. **同步开销**：协调多个处理单元的执行
4. **内存一致性**：保证共享数据的正确访问
5. **可扩展性**：随着处理单元增加的性能提升

## 多核处理器架构

多核处理器是在单个芯片上集成多个处理核心的处理器，是实现并行计算的主要硬件平台。

### 多核处理器设计

```c
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <stdint.h>
#include <stdbool.h>
#include <math.h>

// 核心类型
typedef enum {
    CORE_TYPE_HOMOGENEOUS,   // 同构多核
    CORE_TYPE_HETEROGENEOUS, // 异构多核
    CORE_TYPE_HYBRID         // 混合多核
} CoreType;

// 缓存一致性协议
typedef enum {
    COHERENCE_MESI,          // MESI协议
    COHERENCE_MOSI,          // MOSI协议
    COHERENCE_DRAGON,        // Dragon协议
    COHERENCE_FIRE_FLY,      // Firefly协议
    COHERENCE_DIRECTORY      // 目录协议
} CoherenceProtocol;

// 互连网络拓扑
typedef enum {
    TOPOLOGY_BUS,            // 总线
    TOPOLOGY_RING,           // 环形
    TOPOLOGY_MESH,           // 网格
    TOPOLOGY_TORUS,          // 环面
    TOPOLOGY_CROSSBAR,       // 交叉开关
    TOPOLOGY_TREE            // 树形
} NetworkTopology;

// 核心配置
typedef struct {
    uint32_t core_id;         // 核心ID
    uint32_t frequency;       // 频率(MHz)
    uint32_t l1_cache_size;  // L1缓存大小(KB)
    uint32_t l2_cache_size;  // L2缓存大小(KB)
    uint32_t instruction_units; // 指令单元数
    uint32_t pipeline_depth;  // 流水线深度
    bool supports_smt;        // 支持SMT
    uint32_t smt_threads;     // SMT线程数
} CoreConfig;

// 多核处理器配置
typedef struct {
    CoreType type;            // 核心类型
    uint32_t num_cores;       // 核心数量
    CoherenceProtocol coherence; // 一致性协议
    NetworkTopology topology; // 拓扑结构
    uint32_t l3_cache_size;   // L3缓存大小(MB)
    uint32_t memory_channels; // 内存通道数
    uint32_t memory_bandwidth; // 内存带宽(GB/s)
    uint32_t tdp;             // 热设计功耗(W)
    CoreConfig* cores;        // 核心配置数组
} MultiCoreConfig;

// 性能指标
typedef struct {
    double throughput;        // 吞吐量
    double latency;           // 延迟
    double power_efficiency;  // 功效比
    double scalability;       // 可扩展性
    double cache_hit_rate;    // 缓存命中率
    double utilization;       // 利用率
    uint32_t thermal_cycles;  // 热循环次数
} PerformanceMetrics;

// 多核处理器模拟器
typedef struct {
    MultiCoreConfig config;
    PerformanceMetrics metrics;
    uint32_t* core_states;     // 核心状态
    uint32_t* cache_states;    // 缓存状态
    double* power_consumption;  // 功耗
    uint32_t time_step;        // 时间步长
} MultiCoreSimulator;

// 初始化多核处理器配置
MultiCoreConfig* init_multicore_config(uint32_t num_cores, CoreType type) {
    MultiCoreConfig* config = malloc(sizeof(MultiCoreConfig));
    config->type = type;
    config->num_cores = num_cores;
    config->coherence = COHERENCE_MESI;
    config->topology = TOPOLOGY_MESH;
    config->l3_cache_size = 8;  // 8MB
    config->memory_channels = 2;
    config->memory_bandwidth = 25;  // 25GB/s
    config->tdp = 65;  // 65W

    config->cores = malloc(num_cores * sizeof(CoreConfig));
    for (uint32_t i = 0; i < num_cores; i++) {
        config->cores[i].core_id = i;
        config->cores[i].frequency = 3000;  // 3GHz
        config->cores[i].l1_cache_size = 32;  // 32KB
        config->cores[i].l2_cache_size = 256; // 256KB
        config->cores[i].instruction_units = 6;
        config->cores[i].pipeline_depth = 14;
        config->cores[i].supports_smt = true;
        config->cores[i].smt_threads = 2;
    }

    return config;
}

// 初始化多核处理器模拟器
MultiCoreSimulator* init_multicore_simulator(MultiCoreConfig* config) {
    MultiCoreSimulator* sim = malloc(sizeof(MultiCoreSimulator));
    sim->config = *config;
    sim->time_step = 0;

    sim->core_states = malloc(config->num_cores * sizeof(uint32_t));
    sim->cache_states = malloc(config->num_cores * sizeof(uint32_t));
    sim->power_consumption = malloc(config->num_cores * sizeof(double));

    for (uint32_t i = 0; i < config->num_cores; i++) {
        sim->core_states[i] = 0;  // 空闲状态
        sim->cache_states[i] = 0; // 缓存状态
        sim->power_consumption[i] = 0.0;
    }

    memset(&sim->metrics, 0, sizeof(PerformanceMetrics));

    return sim;
}

// 计算处理器性能
void calculate_processor_performance(MultiCoreSimulator* sim) {
    uint32_t active_cores = 0;
    double total_throughput = 0.0;
    double total_power = 0.0;

    for (uint32_t i = 0; i < sim->config.num_cores; i++) {
        if (sim->core_states[i] != 0) {
            active_cores++;
            total_throughput += sim->config.cores[i].frequency / 1000.0;  // GHz
            total_power += sim->power_consumption[i];
        }
    }

    sim->metrics.throughput = total_throughput;
    sim->metrics.utilization = (double)active_cores / sim->config.num_cores;
    sim->metrics.power_efficiency = total_throughput / total_power;
}

// 模拟核心调度
void simulate_core_scheduling(MultiCoreSimulator* sim, uint32_t* task_assignment) {
    for (uint32_t i = 0; i < sim->config.num_cores; i++) {
        sim->core_states[i] = task_assignment[i];

        // 计算功耗
        if (task_assignment[i] > 0) {
            double base_power = 10.0;  // 基础功耗
            double dynamic_power = task_assignment[i] * 0.5;  // 动态功耗
            sim->power_consumption[i] = base_power + dynamic_power;
        } else {
            sim->power_consumption[i] = 2.0;  // 空闲功耗
        }
    }

    calculate_processor_performance(sim);
}

// 分析缓存一致性开销
double analyze_coherence_overhead(MultiCoreSimulator* sim) {
    double overhead = 0.0;

    switch (sim->config.coherence) {
        case COHERENCE_MESI:
            overhead = sim->config.num_cores * 0.05;
            break;
        case COHERENCE_DIRECTORY:
            overhead = sim->config.num_cores * 0.03;
            break;
        default:
            overhead = sim->config.num_cores * 0.04;
    }

    return overhead;
}

// 分析网络拓扑性能
double analyze_network_performance(MultiCoreSimulator* sim) {
    double latency = 0.0;
    double bandwidth = 0.0;

    switch (sim->config.topology) {
        case TOPOLOGY_BUS:
            latency = sim->config.num_cores * 2.0;
            bandwidth = 10.0 / sim->config.num_cores;
            break;
        case TOPOLOGY_MESH:
            latency = sqrt(sim->config.num_cores) * 1.5;
            bandwidth = sim->config.num_cores * 2.0;
            break;
        case TOPOLOGY_CROSSBAR:
            latency = 1.0;
            bandwidth = sim->config.num_cores * 5.0;
            break;
        default:
            latency = sqrt(sim->config.num_cores) * 2.0;
            bandwidth = sim->config.num_cores * 1.5;
    }

    return bandwidth / latency;
}

// 多核处理器架构分析
void analyze_multicore_architectures() {
    printf("多核处理器架构分析\n");
    printf("==================\n");

    struct {
        CoreType type;
        const char* name;
        const char* description;
        const char* examples;
        double efficiency;
        double flexibility;
    } architectures[] = {
        {CORE_TYPE_HOMOGENEOUS, "同构多核", "所有核心相同", "Intel Core i7", 0.8, 0.6},
        {CORE_TYPE_HETEROGENEOUS, "异构多核", "不同类型核心", "ARM big.LITTLE", 0.9, 0.8},
        {CORE_TYPE_HYBRID, "混合多核", "结合CPU和加速器", "Intel Lakefield", 0.85, 0.9}
    };

    printf("类型\t名称\t描述\t\t示例\t\t效率\t灵活性\n");
    printf("----\t----\t----\t\t----\t\t----\t----\n");

    for (int i = 0; i < 3; i++) {
        printf("%s\t%s\t%s\t%s\t%.2f\t%.2f\n",
               architectures[i].name, architectures[i].name,
               architectures[i].description, architectures[i].examples,
               architectures[i].efficiency, architectures[i].flexibility);
    }
}

// 缓存一致性协议分析
void analyze_coherence_protocols() {
    printf("\n缓存一致性协议分析\n");
    printf("==================\n");

    struct {
        CoherenceProtocol protocol;
        const char* name;
        const char* description;
        const char* advantages;
        const char* disadvantages;
        double overhead;
    } protocols[] = {
        {COHERENCE_MESI, "MESI", "修改-独占-共享-无效",
         "简单高效", "扩展性有限", 0.05},
        {COHERENCE_MOSI, "MOSI", "修改-独占-共享-无效-拥有",
         "减少写回", "复杂度增加", 0.06},
        {COHERENCE_DIRECTORY, "目录协议", "集中式目录管理",
         "高扩展性", "目录成为瓶颈", 0.03},
        {COHERENCE_DRAGON, "Dragon", "基于状态的协议",
         "低延迟", "实现复杂", 0.07}
    };

    printf("协议\t名称\t描述\t\t优势\t\t劣势\t\t开销\n");
    printf("----\t----\t----\t\t----\t\t----\t\t----\n");

    for (int i = 0; i < 4; i++) {
        printf("%s\t%s\t%s\t%s\t%s\t%.2f\n",
               protocols[i].name, protocols[i].name,
               protocols[i].description, protocols[i].advantages,
               protocols[i].disadvantages, protocols[i].overhead);
    }
}

// 互连网络拓扑分析
void analyze_network_topologies() {
    printf("\n互连网络拓扑分析\n");
    printf("================\n");

    struct {
        NetworkTopology topology;
        const char* name;
        const char* description;
        uint32_t max_nodes;
        double latency;
        double bandwidth;
        double cost;
    } topologies[] = {
        {TOPOLOGY_BUS, "总线", "所有节点共享总线", 8, 10.0, 1.0, 0.1},
        {TOPOLOGY_RING, "环形", "节点连接成环", 16, 8.0, 2.0, 0.2},
        {TOPOLOGY_MESH, "网格", "2D网格连接", 64, 4.0, 4.0, 0.4},
        {TOPOLOGY_TORUS, "环面", "带环连接的网格", 256, 3.0, 8.0, 0.8},
        {TOPOLOGY_CROSSBAR, "交叉开关", "全连接", 1024, 1.0, 16.0, 1.0}
    };

    printf("拓扑\t名称\t描述\t\t最大节点\t延迟\t带宽\t成本\n");
    printf("----\t----\t----\t\t----\t----\t----\t----\n");

    for (int i = 0; i < 5; i++) {
        printf("%s\t%s\t%s\t%u\t%.1f\t%.1f\t%.1f\n",
               topologies[i].name, topologies[i].name,
               topologies[i].description, topologies[i].max_nodes,
               topologies[i].latency, topologies[i].bandwidth, topologies[i].cost);
    }
}

// 功耗和热管理分析
void analyze_power_thermal_management() {
    printf("\n功耗和热管理分析\n");
    printf("================\n");

    printf("功耗组成：\n");
    printf("1. 静态功耗：漏电流功耗，与温度相关\n");
    printf("2. 动态功耗：开关功耗，与频率和电压相关\n");
    printf("3. 短路功耗：晶体管短路电流\n\n");

    printf("功耗优化技术：\n");
    printf("- 动态电压频率调节(DVFS)\n");
    printf("- 门控时钟(Clock Gating)\n");
    printf("- 电源门控(Power Gating)\n");
    printf("- 异核设计(big.LITTLE)\n\n");

    printf("热管理策略：\n");
    printf("- 动态温度管理(DTM)\n");
    printf("- 任务迁移\n");
    printf("- 频率节流\n");
    printf("- 散热设计优化\n");
}

// 模拟多核处理器工作负载
void simulate_multicore_workload() {
    printf("\n多核处理器工作负载模拟\n");
    printf("======================\n");

    // 创建8核处理器
    MultiCoreConfig* config = init_multicore_config(8, CORE_TYPE_HOMOGENEOUS);
    MultiCoreSimulator* sim = init_multicore_simulator(config);

    // 模拟不同工作负载
    uint32_t workloads[][8] = {
        {1, 1, 1, 1, 1, 1, 1, 1},  // 均匀负载
        {8, 0, 0, 0, 0, 0, 0, 0},  // 单核心负载
        {4, 4, 0, 0, 0, 0, 0, 0},  // 双核心负载
        {2, 2, 2, 2, 2, 2, 2, 2},  // 轻负载
        {8, 8, 8, 8, 0, 0, 0, 0},  // 部分重负载
        {8, 8, 8, 8, 8, 8, 8, 8}   // 满负载
    };

    const char* workload_names[] = {
        "均匀负载", "单核心", "双核心", "轻负载", "部分重负载", "满负载"
    };

    for (int i = 0; i < 6; i++) {
        printf("\n%s:\n", workload_names[i]);
        simulate_core_scheduling(sim, workloads[i]);

        printf("  吞吐量: %.2f GHz\n", sim->metrics.throughput);
        printf("  利用率: %.1f%%\n", sim->metrics.utilization * 100);
        printf("  功效比: %.2f GHz/W\n", sim->metrics.power_efficiency);

        double total_power = 0.0;
        for (uint32_t j = 0; j < config->num_cores; j++) {
            total_power += sim->power_consumption[j];
        }
        printf("  总功耗: %.2f W\n", total_power);
    }

    // 分析网络性能
    double network_perf = analyze_network_performance(sim);
    printf("\n网络性能: %.2f\n", network_perf);

    // 分析一致性开销
    double coherence_overhead = analyze_coherence_overhead(sim);
    printf("一致性开销: %.2f%%\n", coherence_overhead * 100);

    // 清理资源
    free(config->cores);
    free(config);
    free(sim->core_states);
    free(sim->cache_states);
    free(sim->power_consumption);
    free(sim);
}

// 主函数
int main() {
    printf("多核处理器架构分析器\n");
    printf("===================\n\n");

    // 分析多核架构
    analyze_multicore_architectures();

    // 分析缓存一致性协议
    analyze_coherence_protocols();

    // 分析网络拓扑
    analyze_network_topologies();

    // 分析功耗热管理
    analyze_power_thermal_management();

    // 模拟工作负载
    simulate_multicore_workload();

    return 0;
}
```

### 多核处理器的设计挑战

1. **缓存一致性**：维护多核间数据一致性
2. **通信开销**：核心间数据传输成本
3. **功耗管理**：动态功耗控制和热管理
4. **负载均衡**：任务分配和调度
5. **可扩展性**：核心数量的扩展限制

## 线程级并行

线程级并行是利用多线程技术实现并行计算的主要方式，包括共享内存和分布式内存两种模型。

### 多线程编程模型

```c
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <stdint.h>
#include <stdbool.h>
#include <pthread.h>
#include <time.h>
#include <math.h>

// 线程状态
typedef enum {
    THREAD_STATE_CREATED,    // 已创建
    THREAD_STATE_READY,      // 就绪
    THREAD_STATE_RUNNING,     // 运行
    THREAD_STATE_BLOCKED,     // 阻塞
    THREAD_STATE_TERMINATED   // 终止
} ThreadState;

// 同步原语类型
typedef enum {
    SYNC_MUTEX,              // 互斥锁
    SYNC_SEMAPHORE,          // 信号量
    SYNC_CONDITION,          // 条件变量
    SYNC_BARRIER,            // 屏障
    SYNC_RWLOCK              // 读写锁
} SyncPrimitiveType;

// 线程模型
typedef enum {
    THREAD_MODEL_PTHREADS,   // POSIX线程
    THREAD_MODEL_OPENMP,     // OpenMP
    THREAD_MODEL_CILK,       // Cilk
    THREAD_MODEL_TBB         // Intel TBB
} ThreadModel;

// 任务类型
typedef struct {
    uint32_t task_id;        // 任务ID
    void (*function)(void*); // 任务函数
    void* argument;          // 任务参数
    uint32_t priority;       // 优先级
    uint32_t dependencies;    // 依赖关系
    uint32_t estimated_time; // 预估时间
    bool completed;          // 完成标志
} Task;

// 线程信息
typedef struct {
    pthread_t handle;        // 线程句柄
    uint32_t thread_id;      // 线程ID
    ThreadState state;       // 线程状态
    uint32_t cpu_affinity;   // CPU亲和性
    uint32_t priority;       // 优先级
    double cpu_time;         // CPU时间
    uint32_t tasks_completed; // 完成任务数
    void* result;            // 执行结果
} ThreadInfo;

// 线程池配置
typedef struct {
    uint32_t num_threads;    // 线程数量
    uint32_t queue_size;     // 任务队列大小
    ThreadModel model;       // 线程模型
    bool dynamic_scaling;    // 动态扩展
    uint32_t min_threads;    // 最小线程数
    uint32_t max_threads;    // 最大线程数
    uint32_t idle_timeout;   // 空闲超时
} ThreadPoolConfig;

// 任务队列
typedef struct {
    Task** tasks;            // 任务数组
    uint32_t capacity;       // 队列容量
    uint32_t size;           // 当前大小
    uint32_t head;           // 队头
    uint32_t tail;           // 队尾
    pthread_mutex_t mutex;   // 互斥锁
    pthread_cond_t not_empty; // 非空条件
    pthread_cond_t not_full;  // 非满条件
} TaskQueue;

// 线程池
typedef struct {
    ThreadPoolConfig config;  // 配置
    ThreadInfo* threads;      // 线程信息数组
    TaskQueue* task_queue;   // 任务队列
    bool running;            // 运行标志
    uint32_t active_threads; // 活动线程数
    uint64_t total_tasks;    // 总任务数
    uint64_t completed_tasks; // 完成任务数
    pthread_mutex_t stats_mutex; // 统计互斥锁
} ThreadPool;

// 同步原语基类
typedef struct {
    SyncPrimitiveType type;   // 类型
    pthread_mutex_t mutex;   // 内部互斥锁
} SyncPrimitive;

// 互斥锁
typedef struct {
    SyncPrimitive base;      // 基类
    pthread_mutex_t mutex;   // 互斥锁
    uint32_t owner;          // 拥有者
    uint32_t lock_count;     // 锁计数
} Mutex;

// 信号量
typedef struct {
    SyncPrimitive base;      // 基类
    uint32_t value;          // 信号量值
    uint32_t max_value;     // 最大值
    pthread_cond_t cond;     // 条件变量
} Semaphore;

// 屏障
typedef struct {
    SyncPrimitive base;      // 基类
    uint32_t count;          // 计数器
    uint32_t expected;       // 期望值
    pthread_cond_t cond;     // 条件变量
} Barrier;

// 初始化任务队列
TaskQueue* init_task_queue(uint32_t capacity) {
    TaskQueue* queue = malloc(sizeof(TaskQueue));
    queue->tasks = malloc(capacity * sizeof(Task*));
    queue->capacity = capacity;
    queue->size = 0;
    queue->head = 0;
    queue->tail = 0;

    pthread_mutex_init(&queue->mutex, NULL);
    pthread_cond_init(&queue->not_empty, NULL);
    pthread_cond_init(&queue->not_full, NULL);

    return queue;
}

// 入队任务
bool enqueue_task(TaskQueue* queue, Task* task) {
    pthread_mutex_lock(&queue->mutex);

    // 等待队列不满
    while (queue->size >= queue->capacity) {
        pthread_cond_wait(&queue->not_full, &queue->mutex);
    }

    queue->tasks[queue->tail] = task;
    queue->tail = (queue->tail + 1) % queue->capacity;
    queue->size++;

    pthread_cond_signal(&queue->not_empty);
    pthread_mutex_unlock(&queue->mutex);

    return true;
}

// 出队任务
Task* dequeue_task(TaskQueue* queue) {
    pthread_mutex_lock(&queue->mutex);

    // 等待队列不空
    while (queue->size == 0) {
        pthread_cond_wait(&queue->not_empty, &queue->mutex);
    }

    Task* task = queue->tasks[queue->head];
    queue->head = (queue->head + 1) % queue->capacity;
    queue->size--;

    pthread_cond_signal(&queue->not_full);
    pthread_mutex_unlock(&queue->mutex);

    return task;
}

// 初始化互斥锁
Mutex* init_mutex() {
    Mutex* mutex = malloc(sizeof(Mutex));
    mutex->base.type = SYNC_MUTEX;
    pthread_mutex_init(&mutex->mutex, NULL);
    mutex->owner = 0;
    mutex->lock_count = 0;
    return mutex;
}

// 加锁
void mutex_lock(Mutex* mutex) {
    pthread_mutex_lock(&mutex->mutex);
    mutex->lock_count++;
    mutex->owner = pthread_self();
}

// 解锁
void mutex_unlock(Mutex* mutex) {
    if (mutex->owner == pthread_self()) {
        mutex->lock_count--;
        if (mutex->lock_count == 0) {
            mutex->owner = 0;
        }
    }
    pthread_mutex_unlock(&mutex->mutex);
}

// 初始化信号量
Semaphore* init_semaphore(uint32_t initial_value, uint32_t max_value) {
    Semaphore* sem = malloc(sizeof(Semaphore));
    sem->base.type = SYNC_SEMAPHORE;
    sem->value = initial_value;
    sem->max_value = max_value;
    pthread_cond_init(&sem->cond, NULL);
    return sem;
}

// 等待信号量
void semaphore_wait(Semaphore* sem) {
    pthread_mutex_lock(&sem->base.mutex);
    while (sem->value == 0) {
        pthread_cond_wait(&sem->cond, &sem->base.mutex);
    }
    sem->value--;
    pthread_mutex_unlock(&sem->base.mutex);
}

// 发送信号量
void semaphore_post(Semaphore* sem) {
    pthread_mutex_lock(&sem->base.mutex);
    if (sem->value < sem->max_value) {
        sem->value++;
        pthread_cond_signal(&sem->cond);
    }
    pthread_mutex_unlock(&sem->base.mutex);
}

// 初始化屏障
Barrier* init_barrier(uint32_t count) {
    Barrier* barrier = malloc(sizeof(Barrier));
    barrier->base.type = SYNC_BARRIER;
    barrier->count = 0;
    barrier->expected = count;
    pthread_cond_init(&barrier->cond, NULL);
    return barrier;
}

// 等待屏障
void barrier_wait(Barrier* barrier) {
    pthread_mutex_lock(&barrier->base.mutex);
    barrier->count++;
    if (barrier->count == barrier->expected) {
        barrier->count = 0;
        pthread_cond_broadcast(&barrier->cond);
    } else {
        pthread_cond_wait(&barrier->cond, &barrier->base.mutex);
    }
    pthread_mutex_unlock(&barrier->base.mutex);
}

// 线程工作函数
void* thread_worker(void* arg) {
    ThreadPool* pool = (ThreadPool*)arg;
    ThreadInfo* info = NULL;

    // 找到对应的线程信息
    for (uint32_t i = 0; i < pool->config.num_threads; i++) {
        if (pthread_equal(pthread_self(), pool->threads[i].handle)) {
            info = &pool->threads[i];
            break;
        }
    }

    if (info) {
        info->state = THREAD_STATE_RUNNING;
    }

    while (pool->running) {
        // 从队列获取任务
        Task* task = dequeue_task(pool->task_queue);
        if (!task) break;

        if (info) {
            info->state = THREAD_STATE_RUNNING;
        }

        // 执行任务
        clock_t start = clock();
        task->function(task->argument);
        clock_t end = clock();

        task->completed = true;

        // 更新统计信息
        pthread_mutex_lock(&pool->stats_mutex);
        pool->completed_tasks++;
        if (info) {
            info->tasks_completed++;
            info->cpu_time += (double)(end - start) / CLOCKS_PER_SEC;
        }
        pthread_mutex_unlock(&pool->stats_mutex);

        // 释放任务
        free(task);
    }

    if (info) {
        info->state = THREAD_STATE_TERMINATED;
    }

    return NULL;
}

// 初始化线程池
ThreadPool* init_thread_pool(ThreadPoolConfig* config) {
    ThreadPool* pool = malloc(sizeof(ThreadPool));
    pool->config = *config;
    pool->running = true;
    pool->active_threads = 0;
    pool->total_tasks = 0;
    pool->completed_tasks = 0;

    // 初始化任务队列
    pool->task_queue = init_task_queue(config->queue_size);

    // 初始化线程信息
    pool->threads = malloc(config->num_threads * sizeof(ThreadInfo));
    for (uint32_t i = 0; i < config->num_threads; i++) {
        pool->threads[i].thread_id = i;
        pool->threads[i].state = THREAD_STATE_CREATED;
        pool->threads[i].cpu_affinity = i;
        pool->threads[i].priority = 0;
        pool->threads[i].cpu_time = 0.0;
        pool->threads[i].tasks_completed = 0;
        pool->threads[i].result = NULL;

        // 创建线程
        pthread_create(&pool->threads[i].handle, NULL, thread_worker, pool);
    }

    pthread_mutex_init(&pool->stats_mutex, NULL);

    return pool;
}

// 提交任务到线程池
bool submit_task(ThreadPool* pool, void (*function)(void*), void* argument) {
    Task* task = malloc(sizeof(Task));
    task->function = function;
    task->argument = argument;
    task->priority = 0;
    task->dependencies = 0;
    task->estimated_time = 0;
    task->completed = false;

    pthread_mutex_lock(&pool->stats_mutex);
    pool->total_tasks++;
    pthread_mutex_unlock(&pool->stats_mutex);

    return enqueue_task(pool->task_queue, task);
}

// 等待线程池完成所有任务
void wait_for_completion(ThreadPool* pool) {
    while (pool->completed_tasks < pool->total_tasks) {
        usleep(1000);  // 1ms
    }
}

// 销毁线程池
void destroy_thread_pool(ThreadPool* pool) {
    pool->running = false;

    // 等待所有线程完成
    for (uint32_t i = 0; i < pool->config.num_threads; i++) {
        pthread_join(pool->threads[i].handle, NULL);
    }

    // 清理资源
    free(pool->threads);
    free(pool->task_queue->tasks);
    free(pool->task_queue);
    pthread_mutex_destroy(&pool->stats_mutex);
    free(pool);
}

// 分析线程模型
void analyze_thread_models() {
    printf("线程模型分析\n");
    printf("============\n");

    struct {
        ThreadModel model;
        const char* name;
        const char* description;
        const char* advantages;
        const char* disadvantages;
        double overhead;
    } models[] = {
        {THREAD_MODEL_PTHREADS, "POSIX线程", "底层线程接口",
         "完全控制，跨平台", "编程复杂", 0.3},
        {THREAD_MODEL_OPENMP, "OpenMP", "编译器指令",
         "易用性强，自动并行", "控制有限", 0.2},
        {THREAD_MODEL_CILK, "Cilk", "C语言扩展",
         "工作窃取，高效", "非标准", 0.25},
        {THREAD_MODEL_TBB, "Intel TBB", "C++模板库",
         "任务并行，高级抽象", "C++专用", 0.4}
    };

    printf("模型\t名称\t描述\t\t优势\t\t劣势\t\t开销\n");
    printf("----\t----\t----\t\t----\t\t----\t\t----\n");

    for (int i = 0; i < 4; i++) {
        printf("%s\t%s\t%s\t%s\t%s\t%.2f\n",
               models[i].name, models[i].name,
               models[i].description, models[i].advantages,
               models[i].disadvantages, models[i].overhead);
    }
}

// 分析同步原语
void analyze_synchronization_primitives() {
    printf("\n同步原语分析\n");
    printf("==============\n");

    struct {
        SyncPrimitiveType type;
        const char* name;
        const char* use_case;
        const char* performance;
        const char* complexity;
    } primitives[] = {
        {SYNC_MUTEX, "互斥锁", "独占访问", "高", "低"},
        {SYNC_SEMAPHORE, "信号量", "资源计数", "中", "中"},
        {SYNC_CONDITION, "条件变量", "事件等待", "中", "高"},
        {SYNC_BARRIER, "屏障", "同步点", "低", "低"},
        {SYNC_RWLOCK, "读写锁", "读写分离", "中", "高"}
    };

    printf("类型\t名称\t用途\t\t性能\t复杂度\n");
    printf("----\t----\t----\t\t----\t----\n");

    for (int i = 0; i < 5; i++) {
        printf("%s\t%s\t%s\t%s\t%s\n",
               primitives[i].name, primitives[i].name,
               primitives[i].use_case, primitives[i].performance,
               primitives[i].complexity);
    }
}

// 线程安全分析
void analyze_thread_safety() {
    printf("\n线程安全分析\n");
    printf("============\n");

    printf("线程安全问题：\n");
    printf("1. 竞争条件：多个线程同时访问共享数据\n");
    printf("2. 死锁：线程相互等待对方释放资源\n");
    printf("3. 活锁：线程不断重试但无法进展\n");
    printf("4. 饥饿：某些线程无法获得资源\n\n");

    printf("线程安全策略：\n");
    printf("- 同步机制：锁、信号量、条件变量\n");
    printf("- 原子操作：无锁编程技术\n");
    printf("- 不可变数据：避免共享状态\n");
    printf("- 消息传递：避免共享内存\n");
}

// 测试任务函数
void test_task(void* arg) {
    int* value = (int*)arg;
    usleep(100000);  // 100ms
    (*value)++;
}

// 线程池性能测试
void test_thread_pool_performance() {
    printf("\n线程池性能测试\n");
    printf("==============\n");

    // 测试不同线程池配置
    uint32_t thread_counts[] = {1, 2, 4, 8, 16};
    uint32_t task_counts[] = {100, 500, 1000, 2000};

    printf("线程数\\任务数\t100\t500\t1000\t2000\n");
    printf("--------\\------\t---\t---\t----\t----\n");

    for (int i = 0; i < 5; i++) {
        printf("%u\t\t", thread_counts[i]);
        for (int j = 0; j < 4; j++) {
            ThreadPoolConfig config = {
                .num_threads = thread_counts[i],
                .queue_size = task_counts[j],
                .model = THREAD_MODEL_PTHREADS,
                .dynamic_scaling = false,
                .min_threads = thread_counts[i],
                .max_threads = thread_counts[i],
                .idle_timeout = 1000
            };

            ThreadPool* pool = init_thread_pool(&config);

            // 创建测试任务
            int* test_data = malloc(task_counts[j] * sizeof(int));
            for (uint32_t k = 0; k < task_counts[j]; k++) {
                test_data[k] = 0;
                submit_task(pool, test_task, &test_data[k]);
            }

            // 等待完成
            wait_for_completion(pool);

            // 计算执行时间
            double total_cpu_time = 0.0;
            for (uint32_t k = 0; k < config.num_threads; k++) {
                total_cpu_time += pool->threads[k].cpu_time;
            }

            printf("%.3f\t", total_cpu_time);

            destroy_thread_pool(pool);
            free(test_data);
        }
        printf("\n");
    }
}

// 主函数
int main() {
    printf("线程级并行分析器\n");
    printf("===============\n\n");

    // 分析线程模型
    analyze_thread_models();

    // 分析同步原语
    analyze_synchronization_primitives();

    // 分析线程安全
    analyze_thread_safety();

    // 线程池性能测试
    test_thread_pool_performance();

    // 创建简单示例
    printf("\n简单线程池示例\n");
    printf("==============\n");

    ThreadPoolConfig config = {
        .num_threads = 4,
        .queue_size = 100,
        .model = THREAD_MODEL_PTHREADS,
        .dynamic_scaling = false,
        .min_threads = 4,
        .max_threads = 4,
        .idle_timeout = 1000
    };

    ThreadPool* pool = init_thread_pool(&config);

    // 提交一些测试任务
    int counter = 0;
    for (int i = 0; i < 10; i++) {
        submit_task(pool, test_task, &counter);
    }

    wait_for_completion(pool);

    printf("完成 %u 个任务，最终计数器值: %d\n",
           pool->completed_tasks, counter);

    destroy_thread_pool(pool);

    return 0;
}
```

### 线程并行的关键问题

1. **同步开销**：线程间同步的额外开销
2. **负载均衡**：任务在多个线程间的分配
3. **缓存一致性**：共享数据的访问冲突
4. **死锁问题**：资源竞争导致的死锁
5. **扩展性限制**：线程数量的性能限制

## 向量化计算

向量化计算利用SIMD（单指令多数据）指令，同时在多个数据元素上执行相同的操作。

### SIMD指令和向量化

```c
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <stdint.h>
#include <stdbool.h>
#include <time.h>
#include <math.h>

// SIMD指令集
typedef enum {
    SIMD_NONE,               // 无SIMD支持
    SIMD_MMX,                // MMX
    SIMD_SSE,                // SSE
    SIMD_SSE2,               // SSE2
    SIMD_SSE3,               // SSE3
    SIMD_SSSE3,              // SSSE3
    SIMD_SSE4_1,             // SSE4.1
    SIMD_SSE4_2,             // SSE4.2
    SIMD_AVX,                // AVX
    SIMD_AVX2,               // AVX2
    SIMD_AVX512,             // AVX-512
    SIMD_NEON                // ARM NEON
} SIMDInstructionSet;

// 向量寄存器类型
typedef enum {
    VECTOR_64BIT,            // 64位向量
    VECTOR_128BIT,           // 128位向量
    VECTOR_256BIT,           // 256位向量
    VECTOR_512BIT            // 512位向量
} VectorRegisterType;

// 数据类型
typedef enum {
    DATA_INT8,               // 8位整数
    DATA_INT16,              // 16位整数
    DATA_INT32,              // 32位整数
    DATA_INT64,              // 64位整数
    DATA_FLOAT32,            // 32位浮点
    DATA_FLOAT64             // 64位浮点
} DataType;

// 向量化配置
typedef struct {
    SIMDInstructionSet isa;  // 指令集
    VectorRegisterType reg_type; // 寄存器类型
    DataType data_type;      // 数据类型
    uint32_t vector_length;  // 向量长度
    bool aligned_access;     // 对齐访问
    bool unroll_loops;       // 循环展开
} VectorizationConfig;

// 向量化性能指标
typedef struct {
    double speedup;           // 加速比
    double efficiency;        // 效率
    uint32_t vector_ops;      // 向量操作数
    uint32_t scalar_ops;      // 标量操作数
    double vectorization_ratio; // 向量化比例
    uint32_t memory_bandwidth; // 内存带宽
    uint32_t compute_throughput; // 计算吞吐量
} VectorizationMetrics;

// 向量化操作类型
typedef enum {
    VEC_OP_ADD,              // 向量加法
    VEC_OP_SUB,              // 向量减法
    VEC_OP_MUL,              // 向量乘法
    VEC_OP_DIV,              // 向量除法
    VEC_OP_DOT,              // 点积
    VEC_OP_MAX,              // 最大值
    VEC_OP_MIN,              // 最小值
    VEC_OP_ABS,              // 绝对值
    VEC_OP_SQRT,             // 平方根
    VEC_OP_CMP_EQ,           // 相等比较
    VEC_OP_CMP_GT,           // 大于比较
    VEC_OP_SHUFFLE,          // 数据重排
    VEC_OP_GATHER,           // 收集操作
    VEC_OP_SCATTER           // 散布操作
} VectorOperationType;

// 向量化分析器
typedef struct {
    VectorizationConfig config;
    VectorizationMetrics metrics;
    uint32_t data_size;      // 数据大小
    uint32_t iterations;     // 迭代次数
    double scalar_time;      // 标量执行时间
    double vector_time;      // 向量执行时间
} VectorizationAnalyzer;

// 获取SIMD信息
SIMDInstructionSet get_available_simd() {
    // 这里简化处理，实际需要检测CPU特性
    return SIMD_AVX2;
}

// 获取向量长度
uint32_t get_vector_length(SIMDInstructionSet isa, DataType data_type) {
    switch (isa) {
        case SIMD_SSE:
        case SIMD_SSE2:
        case SIMD_SSE3:
        case SIMD_SSSE3:
        case SIMD_SSE4_1:
        case SIMD_SSE4_2:
            switch (data_type) {
                case DATA_INT8: return 16;
                case DATA_INT16: return 8;
                case DATA_INT32: return 4;
                case DATA_INT64: return 2;
                case DATA_FLOAT32: return 4;
                case DATA_FLOAT64: return 2;
            }
            break;
        case SIMD_AVX:
        case SIMD_AVX2:
            switch (data_type) {
                case DATA_INT8: return 32;
                case DATA_INT16: return 16;
                case DATA_INT32: return 8;
                case DATA_INT64: return 4;
                case DATA_FLOAT32: return 8;
                case DATA_FLOAT64: return 4;
            }
            break;
        case SIMD_AVX512:
            switch (data_type) {
                case DATA_INT8: return 64;
                case DATA_INT16: return 32;
                case DATA_INT32: return 16;
                case DATA_INT64: return 8;
                case DATA_FLOAT32: return 16;
                case DATA_FLOAT64: return 8;
            }
            break;
        default:
            return 1;
    }
    return 1;
}

// 标量向量加法
void scalar_vector_add(float* a, float* b, float* result, uint32_t size) {
    for (uint32_t i = 0; i < size; i++) {
        result[i] = a[i] + b[i];
    }
}

// 向量化加法（模拟）
void vectorized_add(float* a, float* b, float* result, uint32_t size,
                   VectorizationConfig* config) {
    uint32_t vector_length = get_vector_length(config->isa, DATA_FLOAT32);
    uint32_t i = 0;

    // 向量化部分
    for (; i + vector_length <= size; i += vector_length) {
        // 模拟SIMD指令
        for (uint32_t j = 0; j < vector_length; j++) {
            result[i + j] = a[i + j] + b[i + j];
        }
    }

    // 剩余部分
    for (; i < size; i++) {
        result[i] = a[i] + b[i];
    }
}

// 标量矩阵乘法
void scalar_matrix_multiply(float* a, float* b, float* result,
                           uint32_t m, uint32_t n, uint32_t k) {
    for (uint32_t i = 0; i < m; i++) {
        for (uint32_t j = 0; j < k; j++) {
            result[i * k + j] = 0.0f;
            for (uint32_t l = 0; l < n; l++) {
                result[i * k + j] += a[i * n + l] * b[l * k + j];
            }
        }
    }
}

// 向量化矩阵乘法（模拟）
void vectorized_matrix_multiply(float* a, float* b, float* result,
                               uint32_t m, uint32_t n, uint32_t k,
                               VectorizationConfig* config) {
    uint32_t vector_length = get_vector_length(config->isa, DATA_FLOAT32);

    for (uint32_t i = 0; i < m; i++) {
        for (uint32_t j = 0; j < k; j++) {
            result[i * k + j] = 0.0f;

            // 向量化内积计算
            uint32_t l = 0;
            for (; l + vector_length <= n; l += vector_length) {
                // 模拟SIMD乘加操作
                float sum = 0.0f;
                for (uint32_t v = 0; v < vector_length; v++) {
                    sum += a[i * n + l + v] * b[(l + v) * k + j];
                }
                result[i * k + j] += sum;
            }

            // 剩余部分
            for (; l < n; l++) {
                result[i * k + j] += a[i * n + l] * b[l * k + j];
            }
        }
    }
}

// 测量执行时间
double measure_execution_time(void (*func)(void*), void* arg, uint32_t iterations) {
    clock_t start = clock();
    for (uint32_t i = 0; i < iterations; i++) {
        func(arg);
    }
    clock_t end = clock();
    return (double)(end - start) / CLOCKS_PER_SEC;
}

// 分析SIMD指令集
void analyze_simd_instruction_sets() {
    printf("SIMD指令集分析\n");
    printf("==============\n");

    struct {
        SIMDInstructionSet isa;
        const char* name;
        uint32_t register_bits;
        const char* features;
        double speedup_factor;
    } isas[] = {
        {SIMD_SSE, "SSE", 128, "基本浮点SIMD", 2.0},
        {SIMD_SSE2, "SSE2", 128, "整数SIMD", 2.5},
        {SIMD_SSE3, "SSE3", 128, "水平操作", 2.6},
        {SIMD_SSSE3, "SSSE3", 128, "补充SIMD3", 2.7},
        {SIMD_SSE4_1, "SSE4.1", 128, "向量操作", 3.0},
        {SIMD_SSE4_2, "SSE4.2", 128, "字符串处理", 3.1},
        {SIMD_AVX, "AVX", 256, "高级向量扩展", 4.0},
        {SIMD_AVX2, "AVX2", 256, "AVX整数扩展", 5.0},
        {SIMD_AVX512, "AVX-512", 512, "512位向量", 8.0}
    };

    printf("指令集\t名称\t寄存器位\t特性\t\t加速比\n");
    printf("------\t----\t--------\t----\t\t----\n");

    for (int i = 0; i < 9; i++) {
        printf("%s\t%s\t%u\t\t%s\t%.1fx\n",
               isas[i].name, isas[i].name, isas[i].register_bits,
               isas[i].features, isas[i].speedup_factor);
    }
}

// 分析向量化技术
void analyze_vectorization_techniques() {
    printf("\n向量化技术分析\n");
    printf("================\n");

    printf("1. 自动向量化\n");
    printf("   - 编译器自动识别可并行循环\n");
    printf("   - 需要简单的循环结构\n");
    printf("   - 性能依赖于编译器优化能力\n\n");

    printf("2. 内在函数(Intrinsics)\n");
    printf("   - 直接调用SIMD指令\n");
    printf("   - 更好的控制力\n");
    printf("   - 需要手动优化\n\n");

    printf("3. 汇编级向量化\n");
    printf("   - 直接编写SIMD汇编代码\n");
    printf("   - 最高性能\n");
    printf("   - 复杂且不可移植\n\n");

    printf("4. 向量化库\n");
    printf("   - 使用优化库函数\n");
    printf("   - 高性能且易用\n");
    printf("   - 依赖特定库\n");
}

// 向量化优化策略
void vectorization_optimization_strategies() {
    printf("\n向量化优化策略\n");
    printf("================\n");

    printf("1. 数据对齐\n");
    printf("   - 确保数据按向量长度对齐\n");
    printf("   - 使用对齐内存分配\n");
    printf("   - 避免非对齐访问惩罚\n\n");

    printf("2. 循环展开\n");
    printf("   - 减少循环控制开销\n");
    printf("   - 增加指令级并行\n");
    printf("   - 提高流水线效率\n\n");

    printf("3. 内存访问模式\n");
    printf("   - 连续内存访问\n");
    printf("   - 避免跨cache line访问\n");
    printf("   - 预取数据\n\n");

    printf("4. 数据布局\n");
    printf("   - 结构体数组(Array of Structures)\n");
    printf("   - 数组结构体(Structure of Arrays)\n");
    printf("   - 选择最佳数据组织方式\n\n");

    printf("5. 指令调度\n");
    printf("   - 避免数据依赖\n");
    printf("   - 重叠内存和计算操作\n");
    printf("   - 减少停顿周期\n");
}

// 向量化性能测试
void test_vectorization_performance() {
    printf("\n向量化性能测试\n");
    printf("================\n");

    // 测试向量加法
    uint32_t sizes[] = {1024, 4096, 16384, 65536};
    uint32_t num_tests = sizeof(sizes) / sizeof(sizes[0]);

    printf("数据大小\t标量时间\t向量时间\t加速比\t效率\n");
    printf("--------\t--------\t--------\t----\t----\n");

    for (int i = 0; i < num_tests; i++) {
        uint32_t size = sizes[i];

        // 分配测试数据
        float* a = malloc(size * sizeof(float));
        float* b = malloc(size * sizeof(float));
        float* result1 = malloc(size * sizeof(float));
        float* result2 = malloc(size * sizeof(float));

        // 初始化数据
        for (uint32_t j = 0; j < size; j++) {
            a[j] = (float)rand() / RAND_MAX;
            b[j] = (float)rand() / RAND_MAX;
        }

        // 向量化配置
        VectorizationConfig config = {
            .isa = SIMD_AVX2,
            .reg_type = VECTOR_256BIT,
            .data_type = DATA_FLOAT32,
            .vector_length = 8,
            .aligned_access = true,
            .unroll_loops = true
        };

        // 测试标量版本
        double scalar_time = measure_execution_time(
            (void (*)(void*))scalar_vector_add,
            &(struct {float* a; float* b; float* result; uint32_t size;}){a, b, result1, size},
            100);

        // 测试向量化版本
        double vector_time = measure_execution_time(
            (void (*)(void*))vectorized_add,
            &(struct {float* a; float* b; float* result; uint32_t size; VectorizationConfig* config;})
            {a, b, result2, size, &config},
            100);

        // 计算性能指标
        double speedup = scalar_time / vector_time;
        uint32_t vector_length = get_vector_length(config.isa, DATA_FLOAT32);
        double efficiency = speedup / vector_length;

        printf("%u\t\t%.3f\t\t%.3f\t\t%.2fx\t%.1f%%\n",
               size, scalar_time, vector_time, speedup, efficiency * 100);

        // 验证结果
        bool correct = true;
        for (uint32_t j = 0; j < size; j++) {
            if (fabs(result1[j] - result2[j]) > 1e-6) {
                correct = false;
                break;
            }
        }
        if (!correct) {
            printf("  警告: 结果验证失败\n");
        }

        free(a);
        free(b);
        free(result1);
        free(result2);
    }

    // 测试矩阵乘法
    printf("\n矩阵乘法性能测试 (512x512):\n");
    printf("========================\n");

    uint32_t m = 512, n = 512, k = 512;
    float* matrix_a = malloc(m * n * sizeof(float));
    float* matrix_b = malloc(n * k * sizeof(float));
    float* result1 = malloc(m * k * sizeof(float));
    float* result2 = malloc(m * k * sizeof(float));

    // 初始化矩阵
    for (uint32_t i = 0; i < m * n; i++) {
        matrix_a[i] = (float)rand() / RAND_MAX;
    }
    for (uint32_t i = 0; i < n * k; i++) {
        matrix_b[i] = (float)rand() / RAND_MAX;
    }

    // 测试标量矩阵乘法
    double scalar_mat_time = measure_execution_time(
        (void (*)(void*))scalar_matrix_multiply,
        &(struct {float* a; float* b; float* result; uint32_t m; uint32_t n; uint32_t k;})
        {matrix_a, matrix_b, result1, m, n, k},
        10);

    // 测试向量化矩阵乘法
    VectorizationConfig config = {
        .isa = SIMD_AVX2,
        .reg_type = VECTOR_256BIT,
        .data_type = DATA_FLOAT32,
        .vector_length = 8,
        .aligned_access = true,
        .unroll_loops = true
    };

    double vector_mat_time = measure_execution_time(
        (void (*)(void*))vectorized_matrix_multiply,
        &(struct {float* a; float* b; float* result; uint32_t m; uint32_t n; uint32_t k; VectorizationConfig* config;})
        {matrix_a, matrix_b, result2, m, n, k, &config},
        10);

    double mat_speedup = scalar_mat_time / vector_mat_time;
    printf("标量时间: %.3f s\n", scalar_mat_time);
    printf("向量时间: %.3f s\n", vector_mat_time);
    printf("加速比: %.2fx\n", mat_speedup);

    free(matrix_a);
    free(matrix_b);
    free(result1);
    free(result2);
}

// 主函数
int main() {
    printf("向量化计算分析器\n");
    printf("================\n\n");

    // 检测可用SIMD指令集
    SIMDInstructionSet available_isa = get_available_simd();
    printf("可用SIMD指令集: ");
    switch (available_isa) {
        case SIMD_SSE: printf("SSE\n"); break;
        case SIMD_SSE2: printf("SSE2\n"); break;
        case SIMD_AVX: printf("AVX\n"); break;
        case SIMD_AVX2: printf("AVX2\n"); break;
        case SIMD_AVX512: printf("AVX-512\n"); break;
        default: printf("未知\n"); break;
    }

    // 分析SIMD指令集
    analyze_simd_instruction_sets();

    // 分析向量化技术
    analyze_vectorization_techniques();

    // 向量化优化策略
    vectorization_optimization_strategies();

    // 向量化性能测试
    test_vectorization_performance();

    return 0;
}
```

### 向量化计算的优势

1. **数据级并行**：同时处理多个数据元素
2. **高吞吐量**：充分利用处理器宽度
3. **能效比高**：每个操作处理更多数据
4. **硬件支持**：现代处理器内置SIMD单元
5. **广泛应用**：科学计算、图像处理、机器学习

## 总结

并行计算是现代计算系统提升性能的关键技术，涵盖了从指令级并行到大规模并行计算的各个层次。

### 关键技术回顾

1. **并行计算基础**：Amdahl定律、Gustafson定律、并行架构
2. **多核处理器**：缓存一致性、互连网络、功耗管理
3. **线程级并行**：多线程编程、同步机制、线程池
4. **向量化计算**：SIMD指令、自动向量化、性能优化

### 性能优化策略

- **负载均衡**：合理分配计算任务
- **减少通信**：最小化数据传输开销
- **优化同步**：减少锁竞争和同步开销
- **内存优化**：提高缓存利用率和访问效率
- **算法选择**：选择适合并行化的算法

### 发展趋势

1. **异构计算**：CPU+GPU+专用加速器
2. **机器学习加速**：专用AI芯片和架构
3. **量子计算**：全新的并行计算范式
4. **边缘计算**：分布式并行计算
5. **可重构计算**：动态硬件配置

并行计算将继续作为提升计算性能的主要途径，在科学计算、大数据分析、人工智能等领域发挥重要作用。